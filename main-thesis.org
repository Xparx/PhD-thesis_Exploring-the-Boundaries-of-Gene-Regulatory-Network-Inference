# Time-stamp: <2015-09-28 05:55:19 andreas>
#+OPTIONS: title:t toc:nil todo:t |:t email:nil H:4
#+BIND: org-latex-title-command "\\selectlanguage{english}\n\\frontmatterSU\n\\halftitlepage\n\\maketitle"
#+TITLE: my thesis title
#+DATE: \today
#+AUTHOR: Andreas Tjärnberg
#+EMAIL: andreas.tjarnberg@scilifelab.se
#+KEYWORDS:
#+LANGUAGE: GB_en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.0.50.1 (Org mode 8.3)
#+LATEX_CMD: mkbibtex
#+LATEX_CLASS: thesis-book-SU
#+LATEX_CLASS_OPTIONS: [twoside,11pt]
#+DESCRIPTION:
#+LATEX_HEADER: \subtitle{this is important}
#+LATEX_HEADER_EXTRA: \hbadness=10000
#+LATEX_HEADER_EXTRA: \hfuzz=50pt
#+LATEX_HEADER_EXTRA: \input{glossaries-thesis}


* Empty page                                                        :notitle:
#+begin_src latex :exports results :results latex
%: ----------------------- Cover page back side ------------------------
\newpage
\thispagestyle{empty}
#+end_src

* Abstract                                                          :notitle:
#+begin_abstracts
Biological systems
#+end_abstracts

* Document info                                                     :notitle:
#+begin_src latex :exports results :results latex
\phantom{.}

\vspace{\stretch{1}}

{\fontfamily{verdana}\selectfont
{\scriptsize
\noindent
\copyright Andreas Tjärnberg, Stockholm 2015 % Name of author, location year

\vspace{5mm}
\noindent
ISBN XXX-XX-XXXX-XXX-X % Provided by the library

\vspace{5mm}
\noindent
Printed in Sweden by XXXX, Stockholm 2011 % name of printing company

\noindent
Distributor: Department of XX, Stockholm University % name of department
}
}
\cleardoublepage
#+end_src

* Dedication                                                        :notitle:

#+begin_dedication
#+BEGIN_LaTeX
{\fontfamily{calligra}\selectfont
{\Large

This thesis is dedicated to...

}
}
#+END_LaTeX
#+end_dedication

* List of Papers

#+begin_src latex :exports results :results latex
\vspace{-5pt} % Increase to have a larger space.

The following papers, referred to in the text by their Roman numerals, are included in this thesis.

\vspace{0pt} % Increase to have a larger space before the list is started.


\begin{enumerate}[P{A}PER I: ]
%\begin{enumerate}[I]

\setlength{\itemsep}{3.3mm} % Set the vertical distance between the items

% Suggested order
% Author 1 surname, Author 2 first name initial., Author 1 surname, Author 2 first name
% initial. etc. (Year of publication) Paper main title.
% Paper subtitle. Name of journal in italics, volume(number):page rage
% Example


\item\textbf{Titel}\\
Author1, Author2, \emph{paper}, \textbf{issue}, page (YEAR).\\
DOI: \href{}{}

\end{enumerate}

\noindent
\rule{\linewidth}{0.5mm}

\vspace{2mm}

\noindent
Reprints were made with permission from the publishers.
#+end_src

* Table of content                                                  :notitle:
#+begin_src latex :exports results :results latex
%: ----------------------- Table of contents ------------------------

\setcounter{secnumdepth}{2} % organisational level that receives a numbers
\setcounter{tocdepth}{2}    % print table of contents for level 2
\tableofcontents            % print the table of contents
% levels are: 0 - chapter, 1 - section, 2 - subsection, 3 - subsubsection
#+end_src

* Abbreviations                                                     :notitle:
#+begin_src latex :exports results :results latex
% To create the glossary run the command
% $ makeglossaries main-thesis

%\nomrefpage % to include page numbers after abbrevations

% In the text type "\g" to refer to glossary

% \markboth{\MakeUppercase{\nomname}}{\MakeUppercase{\nomname}}

\begin{footnotesize} % scriptsize(7) < footnotesize(8) < small (9) < normal (10)
\printacronyms[title=Abbreviations]
% \printglossary[type=\acronymtype,title=Abbreviations]
\label{nom} % target name for links to glossary
\end{footnotesize}
#+end_src

* Figures and tables                                                :notitle:
#+begin_src latex :exports results :results latex
\listoffigures	% print list of figures
\listoftables     % print list of tables
#+end_src

* Acknowledgements

I would like to acknowledge...

* Mainmatter                                                        :notitle:
#+begin_src latex :exports results :results latex
\mainmatterSU
#+end_src

* Introduction
# It was clear from early on, when the famous physicist Erwin Schrödinger asked the question "What is life"[[cite:Schrodinger1944]] that the field of biological research would gain attention from not only biologist and biochemists and could benefit from input from a diverse array of fields.

# General what is systems
Understanding biological systems is in part motivated by the need to predict or modify the behaviour of these systems.
Whether it is the patterns of whole societies, the interactions of individual
organisms or the cellular internal and external machinery,
understanding theses systems may help prevent the out brake of a disease or the optimal course of action when something within the system breaks down and needs intervening.
One example would be if we consider cancer to be an unwanted state or breakdown of the cellular functions,
we would wish to be able to intervene and return the system to its natural functioning mode.

# Why do we need to look at things as systems of interactions
One of the common features of the above mentioned systems is that they change over time and
they respond to internal and external changes in non random ways.
Information is transferred and advanced in space and over time to other current and future "younger" individuals or components of the systems,
\ie the system evolves over time.
Another common feature among these systems is that the behaviour of the system can not be trivially understood or predicted by knowing any single "unit" alone,
and often an emergent behaviour can be observed as a result of putting several units in a specific context.
A unit is here a cell, an organism, or a population of organisms or cells.
A human cell, for example, can not be grown in isolation without modifying its internal machinery.
Often to a state that would be considered faulty if it existed in its original environment.
That is, the components the that system is built on is not the complete picture without also incorporating the interactions that they exerts with each other.

# Focus on the cell
The focus of this thesis and the work herein is mainly in the context of trying to understand the intracellular system.

# Motivation for this work
As explained above, the intracellular system can not be viewed as being isolated from the environment, and if it were to be isolated, we could not assume that its behaviour would be the same as in its natural context.
This observation makes the study if this system non trivial.
Changes to the system is not easily induced and isolated or even measured.
# without the introduction of noise or unknown effects.

# While a few components could be studied in detail by traditional biochemical and biophysical approaches, to study all the components that built up the cellular machinery both computational and new theoretical tools would be needed.

# the "The Path Forward" section in [[cite:Rao2001]]
# has some nice notes

# Schrodinger what is life [[cite:Schrodinger1944]]

# What is systems biology? [[cite:Vidal2009]]

* Background

** Biological systems
:PROPERTIES:
:CUSTOM_ID: sec:bio_sys
:END:

# Medical implications and motivation
[[cite:Wolkenhauer2009]]

# Drug discovery

*** Gene regulation

# figure with a picture of regulation, gene transcribed -> RNA translated -> protein -> regulat transcription or RNA translation | RNA regulate transcription or protein.

# Abstraction levels of regulation
# metabolic protein RNA
[[cite:Crampin2006]]
# Also a nice figure of different levels of network representation, figure 2.
#
# gene expression
#
# regularization

# Discussing pitfalls related to inferring interactions based on the genetic interaction properties, section 4.
[[cite:He2009]]

# Different type of regulatory models
[[cite:Rao2001]]
# MODELS OF CELLULAR REGULATION
# Metabolism
# Signal Transduction: Bacterial Chemotaxis
# Genetic Swtiches: DNA regulation
# Gene Expression

# bioloigcal functions
[[cite:Sontag2005]]
# specifically section 4 as a starting point.
# in section 3.1 stability is discussed. and in section 3.2 a motivation why other experts are interested in studying these systems.

# biological networks
#

# [[cite:Kremling2007]] A small note about biologically motived criteria

*** Regulatory networks
[[cite:Rao2001]]

[[cite:Teixeira2013]] yeastract

[[cite:Salgado2013]] Regulon db

# network inference from expression data with regulondb and stochastic model CLS.
[[cite:Faith2007]]


**** Network medicine
# Network medicine see notes
[[cite:Barabasi2011]]

[[cite:Flores2013]]

[[cite:Morel2004]]

# [[cite:Schreiber2000]]
# Might be relevant as he comments on pathway drug effects without a specific target.

** System theory
In this section I will give a general description of a system and extending it to include interdependent variables \ie a network.
I will also introduce [[glspl:ode]] and dynamical systems as a description of a system over changing over time,
and finally this section will give a brief description of properties associated with systems in a [[gls:grn]] framework.

*** System description
:PROPERTIES:
:CUSTOM_ID: sec:system_description
:END:

We can in general describe a systems problem as the mathematical model[[cite:Aster2005]]
#+begin_src latex :exports results :results latex
\begin{equation}
  \Phi(a) = \xi
\end{equation}
which vectorised becomes
\begin{equation}\label{eq:system}
  \Phi(\ba) = \bxi
\end{equation}
#+end_src
\noindent
for a multivariate problem where $\ba$ is the model parameters of the model and $\Phi$ is the function that maps the independent variables, also called regressors,
to the independent variables, or regressands, $\xi$.
For a discrete linear system [[ref:eq:system]] becomes a system of equations to be solved
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:sys_equ}
  \mPhi\ba = \bxi
\end{equation}
where independent variables $\phi_{ij}$ is mapped with parameter $a_j$ to our data $xi_i$.
\begin{equation}
  \begin{bmatrix}
    \phi_{11} & \phi_{21} & \phi_{31}\\
    \phi_{12} & \phi_{22} & \phi_{32}\\
    . & . &. \\
    . & . &. \\
    . & . &. \\
    \phi_{1m} & \phi_{2m} & \phi_{3m}\\
  \end{bmatrix}
  \begin{bmatrix}
    a_1\\a_2\\a_3\\
  \end{bmatrix} =
  \begin{bmatrix}
    \xi_1\\\xi_2\\.\\.\\.\\\xi_m
  \end{bmatrix}
\end{equation}
#+end_src
\noindent
$m$ represent the number of samples recorded or measured.

The inverse problem is the problem of trying to find a set $\ba$ to fit the data $\bxi$ given $\mPhi$.
In machine learning and supervised learning $\Phi$ is the features while $\bxi$ would be classes to be predicted by fitting $\ba$.

For network inference this problem is extended further bu introducing inter-dependencies in \(\ba\),
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:net_inf_linear_sys}
  \begin{bmatrix}
    \phi_{11} & \phi_{21} & \phi_{31}\\
    \phi_{12} & \phi_{22} & \phi_{32}\\
    . & . &. \\
    . & . &. \\
    . & . &. \\
    \phi_{1m} & \phi_{2m} & \phi_{3m}\\
  \end{bmatrix}
  \begin{bmatrix}
    a_{11} & a_{21} & a_{31}\\
    a_{12} & a_{22} & a_{32}\\
    a_{13} & a_{23} & a_{33}\\
  \end{bmatrix} =
  \begin{bmatrix}
    \xi_{11} & \xi_{21} & \xi_{31}\\
    \xi_{12} & \xi_{22} & \xi_{32}\\
    . & . &. \\
    . & . &. \\
    . & . &. \\
    \xi_{1m} & \xi_{2m} & \xi_{3m}\\
  \end{bmatrix}
\end{equation}
#+end_src

*** Dynamical Systems
A dynamical system can be described as a set of instructions between entities that dictates entities change over time.
More specifically, the system describe the rules and interconnection between the entities and how they influence each other based based on those connections.
We can have a general description of this definition, in the discreet time mapping
#+begin_src latex :exports results :results latex
\begin{equation}
  x_{t+\tau} = f(x_t)
\end{equation}
#+end_src
where $x$ represent the state of the system at time $t$ and $\tau$ some discreet time step, often taken $\tau=1$. $f$ is here the rules that evolve the system.
This can be written as the difference eqaution,
#+begin_src latex :exports results :results latex
\begin{equation}
  \begin{array}{lcl}
    \bx_{t+\tau} - \bx_t &=& f(\bx_t) - \bx_t\\
    \Delta \bx(t) &=& g(\bx(t))
  \end{array}
\end{equation}
#+end_src
where $\Delta$ is the difference operator and $\bx$ now represent the state vector.
Another way of modeling evolving systems is the [[gls:ode]] model.
[[Gls:ode]] relate the state of the system to its instantaneous change,
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:ode}
  \dot{\bx} = f(\bx,\bp,t)
\end{equation}
#+end_src
where $\dot{\bx}$ is the time derivative of the states $\bx$, $\bp$ is any input to the system, henceforth called perturbation. $f$ may be any function and $t$ the current time.

*** Systems properties

**** Network motives
It is know that some specific network motifs are highly over represented in biological systems, while others are underrepresented, compared to what would be expected of random networks, which is show by investigating the transcriptional network of \coli[[cite:Salgado2013]].
Especially the [[gls:ffl]] motif is highly over represented.
[[citet:Alon2007]] showed that this specific regulatory motives could serve specific functionality, such as delayed response, short pulse, counters and switches.
Another type of motifs that are often considered in system theoretic approaches is the feedback loops.
Feedback loop can cause highly correlated responses, so called interampatte systems, section [[ref:sec:iaa]]. They may also determine phenotypic patterns due to functioning as hard switches[[cite:Wolkenhauer2005]]. Feedback have been show to be able to describe the behaviour of bacterial chemotaxis[[cite:Yi2000]].

**** Steady states
[[Glspl:ss]] are defined by $\dot{\bx} = 0 \equiv f(\bx_0)$ in ([[ref:eq:ode]]).
The nature of the [[gls:ss]] can be elusidated by analysing the system $f(\bx_0) = 0$.
The solution to this equation, or system of equations in multivariate analysis, is the [[gls:ss]].
For the system $f(\bx_0) = 0$ we can calculate the jacobian, $J$, the partial derivatives of $f$ over the states $\bx$.
The nature of the [[glspl:ss]] can then be derived from the eigenvalues of $J$.
If all eigenvalues real part are negative then the system trajectories will converge to a stable state.
If any eigenvalues real part is positive then an unstable trajectory exist for that state variable that will make the system behave unstable.
A system that is unstable will not converge to a stable state where $\dot{\bx} = 0$.
For a linear system ([[ref:eq:linearsys]]) the solution of $f(\bx_0) = 0$ is always unique, meaing that there exist only one [[gls:ss]] for any linear system. The eigenvalues of $J$ might reveal that this is an unstable [[gls:ss]] and the system will diverge away from this state.

Non linear systems might have more complex descriptions of there function $f(\bx_0) = 0$, with multiple solutions.
This means that the system has multiple [[gls:ss]], where some might correspond to converging states, while others might be unstable [[gls:ss]] that when the system is placed in this state it will naturally diverge from the state. 

The stable [[gls:ss]] property have been incorporated in algorithms[[cite:Zavlanos2011]] and when collecting data[[cite:steady_state_data]] for doing network inference [[ref:sec:net_inf]].
The assumption here is that if biological systems would not be stable,
even random variations would eventually accumulate within the system which would lead to a system collapse[[cite:Kremling2007]].

One simple mechanism in [[glspl:grn]] for maintaining stability is degradation.
As every entity that regulates something else in the system will degrade over time an infinite growth can not be maintained.
This because an equilibrium will be reach depending on the grown rate and degradation rates of the molecules[[cite:Alon2007]].

**** Hierarchical systems
Investigating hierarchies in system may help simplify further analysis.
A dynamical system may work on several different time scales.
The time constant $\tau$ can be derived from the eigenvalues of the jacobian, $J$, in essence estimating the size of the system changes.
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:time-constant}
  \tau_i \equiv \frac{1}{|\Re(\lambda_i)|}
\end{equation}
#+end_src
\noindent
where $\Re(\lambda_i)$ is the real part of eigenvalue $\lambda$ for gene $i$.

Practically, the time constant is calculated for a non linear system around its [[gls:ss]]. Fast and slow modes can be separated either by eigenvalue spectral clustering or by imposing a threshold, $\tau^S$ on the time constant, so that if $\tau_i > \tau^S$, $i$  belongs to the fast modes and to the slow otherwise [[cite:Kremling2007]].

Hierarchical analysis of system dynamics have been used reduce dimensionality of a systems
[[cite:Zagaris2003]].
As well as being the cause of interampatte behaviour of the system[[cite:Nordling2009]].

Time constants dynamics can be viewed as operating on different time windows.
Faster modes than the times observable in the window under observation can be considered as [[gls:ss]] and slower modes can be discarded.

Analysing time dynamics could potentially help determine sampling frequency when doing [[gls:tsd]] analysis as the fast responses could be investigated while assuming slower modes are quasi stable[[cite:is_there_a_citation_for_this]].

**** Interampatte systems
:PROPERTIES:
:CUSTOM_ID: sec:iaa
:END:

Interampatteness is a property of biochemical networks that can be recognised by a high correlated response to system perturbations[[cite:Nordling2009]].
The degree of interampatteness can be calculated as the condition number of the static gain matrix.
#+begin_src latex :exports results :results latex
\begin{equation}
  \glssymbol{k}(\mG) = \frac{\overline{\sigma}}{\underline{\sigma}}
\end{equation}
#+end_src
\noindent
where $\overline{\glssymbol{sigma}}$ is the largest [[gls:sigma]] and $\underline{\glssymbol{sigma}}$ is the smallest [[gls:sigma]].

The response data and perturbation design should be related to the interampatteness of the system under investigation.

** Systems biology

Systems biology mainly concerns itself with finding a description of biology that takes in to account the complex interactions that is typically found within \eg the cellular network.
The problems sought to be solved by a systems biology approach concerns behaviours of cellular networks in the light of specific motifs.
Global structure of interaction networks, such as scale-free ness or small-world properties[[cite:Vidal2009]].
To be able to do this the structure of the network needs to be inferred.
This involves what is commonly known as a "top down" approach, contrasting the "bottom up" approach that traditionally means investigating singular regulatory interactions or a specific bio-molecules properties. When most of the specific details of for example the biochemical reactions are known then a "bottoms up" approach can be appropriate to build up a view of the system ind investigate emergent behaviour not observed or easily infer from the parts of the system[[cite:Kremling2007]].

This section will focus on a sub part of what is recognised as systems biology, namely the inference of causal network models describing \acrlong{grn}.

First a brief overview of different model formalism, second a more focused in depth view of linear models and third its application to network inference of [[glspl:grn]].

*** Model formalism
As described in section [[ref:sec:system_description]] we can describe a system generally as [[ref:eq:system]].
Depending on the transfer function and response we can describe several different types of system regularly used in systems biology.

A whole slew of different approaches have been developed or adapted for network inference of [[glspl:grn]]. 
Correlation based methods measure correlated variables and infer a link between genes,
to be able to use correlation based method to infer a directed regulatory network,
and not just an association network, [[gls:tsd]] needs to be used.
# what about partial correlations?

An associated approach is the information theoretic approach.
The information theoretic approach is based on estimating the mutual information of the variation in the expression patterns of measured genes.
The expression space could either be discretised to simplify calculations or used as is.
This type of model extends to non linear relationships as mutual information can describe many types behaviours.

Boolean networks links gene expression through boolean operators such as =AND=, =OR= and =NOT=.
Boolean interactions are based on the truth table of the interactors.
This means that the expression of each gene needs to be discretised to determine if the gene is =ON= or =OFF= and can be expressed as,
#+begin_src latex :exports results :results latex
\begin{equation}
  \bx(t+1) = f^B(\bx(t))
\end{equation}
#+end_src
where $f^B$ is a boolean function and $\bx(t+1)$ is the state of the state variables (=ON= / =OFF=) at time $t+1$ as a function of the state at time $t$.

Bayesian models is by their nature probabilistic.
The models are based on conditional probabilities.
Due to the nature of conditional probabilities the bayesian model can not handle feedbacks.
Not until extended to dynamic baysian models would it be possible to model [[glspl:grn]] with feedbacks.
The Bayesian model is modelled with conditional probabilities
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:bayesian-model}
  \Prob(X_i=x_i|X_j=x_j) = f(x_i|x_j)
\end{equation}
#+end_src
where $x$ represent the specific value of the random variable $X$.
For a network one would evaluate the probability of a structure of relationships.
Each network model would then be a product of conditional probabilities based on the structure of the network.

Another class of models is the [[gls:ode]] models ([[ref:eq:ode]]).
Several different models fall under this umbrella.
An example of a non linear [[gls:ode]] is a model using Michaelis-Menten kinetics. This can be extending to include modelling with the cooperative Hill coefficients. The coefficients in the Hill function can determine the steepness of the activation curve. This could also be replaced in the extreme case with a boolean condition, where activation turns on only if the amount of some activation molecule reaches a certain concentration[[cite:Alon2007]].
# non-linear

# linear models
the linear [[gls:ode]] is a system where the rate of change for e gene in the system is the cumulative effect of all other regulators for that gene. The linear system will be discussed in detail in section [[ref:sec:linear_models]].

There are several review articles describing different approaches and model formalism for network inference in systems biology, see \eg cite:DeJong2002a,Gardner2005,Hecker2009,Yaghoobi2012

# [[cite:Gardner2005]]
# Citation 8 and 12 should detail that linear models have been shown to be more versityle.

*** Linear dynamical models
:PROPERTIES:
:CUSTOM_ID: sec:linear_models
:END:

The draw to use linear models is that they are simple and can describe various complex phenomena observed in biological system,
such as \eg feed back and feed forward motifs.
Even if non linear, as long as the system operates close to [[gls:ss]] a linear model can be employed to describe the casual interactions.

#+begin_src latex :exports results :results latex
\begin{equation}
  \begin{array}{r c l}
    \dot{x}_i(t) &=& \sum_{j=1}^N a_{ij}x_j(t) + p_i(t) - f_i(t)\\
    y_i(t) &=& x_i(t) + e_i(t).
  \end{array}
  \label{eq:linearsys}
\end{equation}
#+end_src
# see \eg \citet{Yuan2011,Gardner2003,Yeung2002}.
#+LATEX: \noindent
If we are using the linear model in a biological systems framework then we would say that the state vector \(\bx(t)=[x_1(t),x_2(t),\ldots,x_N(t)]^T\) represents mRNA expression changes relative to the initial state we refer to as $t-0$ of the system,
the vector \(\bp(t)=[p_1(t),p_2(t),\ldots,p_N(t)]^T\) represents the applied perturbation, which may be corrupted by the noise $\bbf(t)$.
The perturbations could be \eg gene knock-downs using siRNA or gene over-expressions using a plasmid with an extra copy of the gene.
The response vector \(\by(t)=[y_1(t),y_2(t),\ldots,y_N(t)]^T\) represents the measured expression changes that differ from the true expression changes by the noise $\be(t)$.
$a_{ij}$ represents the influence of an expression change of gene $j$ on gene $i$.
If gene $j$ upp regulates gene $i$ then $a_{ij}$ is positive and if gene $j$ down regulates gene $i$ then $a_{ij}$ is negative.
If gene $j$ and $i$ have no interaction then $a_{ij} =0$.

Linear [[gls:ode]] have been extensively used in the context of systems biology.
It has been shown that non linear models can be linearised around a [[gls:ss]] or log-transformed to be able to make use of the properties associated with linear systems and that near [[gls:ss]] the kinetics are well described by a linear model [[cite:Crampin2006]].

**** Steady state data
If we collect only [[gls:ssd]] and use the common notation that each sample is recorded in each column the system will simplify ([[ref:eq:linearsys]]) to
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:Linearmap}
  \mY = -\mA^{-1}\mP +\mA^{-1}\mF + \mE
\end{equation}
#+end_src
#+LATEX: \noindent
when the set of experiments are considered, with $\mY$ being the observed [[gls:ss]] response matrix after applying the perturbations $\mP$ and $\mA$ is the interaction matrix \ie network.
Linear system with steady state data have been used in several network inference projects [[cite:Tegner2003,Gardner2003,Julius2009]].

**** Least squares estimate and prediction error

To find the ordinary least squares estimate for ([[ref:eq:Linearmap]]) one can solve for $\mA$,
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:ls}
  \mA_{ls} = -\mP\mY^{\dagger}
\end{equation}
#+end_src
#+LATEX: \noindent
Here $\dagger$ represent the Moore-Penrose generalised matrix inverse.
In the above equation we assume we can find a solution for $\mA_{ls}$.
However in general, if we have collected noisy data a solution to the above can not be guaranteed. 

To fit the data one wants to find the parameters of the model that minimises the distance to the regression curve that relates the independent and dependent variables[[cite:Aster2005]].
This can be expressed with the following equation,
#+begin_src latex :exports results :results latex
\begin{equation}
  \hat{\mA} = \arg \min_{\mA} ||\mA (\mY-\mE)+(\mP-\mF)||_{L_2}^2
  \label{eq:ols_L2}
\end{equation}
#+end_src
If the noise in $\mF$ and $\mE$ are \iid and normally distributed, $\normall$ with mean $\mu$ and variance, $\lambda$, then the least squares estimate is also the maximum likelihood esitamte[[cite:find_some_citation]].

Equation ([[ref:eq:ols_L2]]) is sensitive to outliers due to the nature of the 2-norm, $\norm{.}_2$ and it might be favourable to introduce the 1-norm instead
#+begin_src latex :exports results :results latex
\begin{equation}
  \hat{\mA} = \arg \min_{\mA} ||\mA (\mY-\mE)+(\mP-\mF)||_{L_1}
  \label{eq:ols_L1}
\end{equation}
#+end_src
this norm corresponds to fitting to the median rather than the mean as in ([[ref:eq:ols_L2]]).
The issue being that while ([[ref:eq:ols_L2]]) is differentiable, ([[ref:eq:ols_L1]]) is not.
This problem can be over come by noting that ([[ref:eq:ols_L1]]) is peace-wise differentiable and convex.
Meaning that one can search for the optimal solution by finding the peace-wise optimal solutions[[cite:Aster2005]].

*** Network inference
:PROPERTIES:
:CUSTOM_ID: sec:net_inf
:END:

# CHECK TORBJORNS THESIS PAGE 28!!!

# Also comment on that biological systems are usualy considered stable [[ref:sec:ss]]

The first objective of network inference is to infer the interaction network between the nodes/genes. The links that describe the causal influence of one entity to another.
[[citet:Gardner2005]] separated two different types of network inference types, the first or "physical" approach aims at construction the transcriptional regulatory network directly, \ie to determine the physical binding of one transcription factor to another. This strategy concerns itself with direct interactions.
In some cases however, it may be that an intermediate step is not observed and no direct binding occurs even though change based on some regulation can be observed.
The other approach is the influence strategy.
Where the regulatory influences are sought.
In this case one can model the network of interactions as in ([[ref:eq:ode]]).
As the primary objective of network inference is to find the regulatory interactions, the problem of network inference is primarily a model identification problem and not a parameter estimation problem.
However, this line is sometimes blurred with the introduction of algorithms such as \lasso[[cite:Tibshirani1996]] which both estimates parameters and also returns a selection of candidate models (see: [[ref:sec:linear_penalty]]).

Several studies have employed a linear dynamical systems framework.
[[citet:Gardner2003]] used a linear model, motivated by linearisation of a non linear model around a [[gls:ss]]. Furthermore data was recorded with a [[gls:ss]] assumption on the measured mRNA expression data for 9 genes in the SOS pathway in \coli. A linear regression method was then used to estimate model parameter and a exhaustively search a subset of interactors for each gene in the network.

A core mechanism to be able to infer a casual influence network from [[gls:ssd]] and a linear dynamical system, section [[ref:sec:linear_models]], is that specific perturbations are made to each gene that is going to be included in the network.
This is the case for [[gls:tsd]] as well with the difference being that for [[gls:tsd]] only a single perturbation needs to be made, and it does not necessarily need to be kept constant until the system relaxes to a [[gls:ss]][[cite:Dhaeseleer1999]].

# Parameter estimation [[cite:Aster2005]]
#
#

**** Penalised linear regression
:PROPERTIES:
:CUSTOM_ID: sec:linear_penalty
:END:
Looking at equation ([[ref:eq:ols_L2]]) and ([[ref:eq:ols_L1]]) it is clear that the estimate of $\check{\mA}_{ols}$ contains contributions from the noise matrices $\mE$ and $\mF$, even if assuming that the independent variable is noise free, $\mF=0$, we still have to deal with a noisy expression matrix $\check{\mY}$.
The result of fitting the data with a noisy $\check{\mY}$, is that the estimated model $\mA_{ols}$ tends to be overfitted, meaning that the paramters of the model fitts the noise.
Classically this have the consequence that the model fitted does not generalise to any other data.
For network inference it means that there is a big chance that a link is inferred in the network which does not exist except for the effect of the noise.
A network like that is hard to interpret as it usually depicts every gene interacting with every other gene[[cite:Hastie2009]].
An early approach of dealing with overfitting was to introduce a peanalty term in the model fitting,
#+begin_src latex :exports results :results latex
\begin{equation}
  \hat{\mA}_{\textrm{reg}}(\tilde{\zeta}) = \arg \min_{\mA} ||\bA \bY+\bP||_{L_2}^2 + \zeta||\bA||_{L_2} .
  \label{eq:ridge-regression}
\end{equation}
#+end_src
Here, $\zeta$ corresponds to a parameter that regulates the impact of the penalty term on the ordinary least squares estimate.
The penalty term $\zeta||\bA||_{L_2}$ penalises the model parameters squared size, this has a result that large parameters will be penalised more than smaller. 
This approach smooths the parameters of the models and as a consequence performs well on ill-conditioned problems. However it does not eliminate model parameters well.

\lasso is another penalty method[[cite:Tibshirani1996]].
The lasso problem can be written as,
#+begin_src latex :exports results :results latex
\begin{equation}
  \hat{\mA}_{\textrm{reg}}(\tilde{\zeta}) = \arg \min_{\mA} ||\bA \bY+\bP||_{L_2}^2 + \tilde{\zeta}||\bA||_{L_1} .
  \label{eq:LASSO}
\end{equation}
#+end_src
the \lasso penalises model parameters absolute size.
The difference from the ridge-regression is that \lasso produces different models depending on the penalty parameter $\zeta$.
\lasso has become popular for network inference due to the fact that it combines model selection as well as parameter estimation.
Due to these properties \lasso has become very popular and a lot of work have been done on the performance of \lasso and modifications of \lasso[[cite:Candes2009,Zhao2006]].
It has been shown that \lasso performs poorly on ill-conditioned data.

As ridge-regression does not suffer from the same weakness as \lasso an effort to combine the both called /elastic-net/ has been made.
The Elastic-net[[cite:Zou2005]] method combines the $L_1$ penalty from \lasso and the $L_2$ penalty from ridge regression. The influence of the penalties are then weighted by a parameter $\alpha$ such that,
#+begin_src latex :exports results :results latex
\begin{equation}
  \hat{\mA}_{\textrm{reg}}(\zeta) = \arg \min_{\mA} C + \tilde{\zeta}\left(\alpha ||\bA||_{L_1} + (1-\alpha)||\bA||_{L_2}^2\right),
  \label{eqn:elastic-net}
\end{equation}
#+end_src
where $C=||\bA \bY+\bP||_{L_2}^2$.

citet:Zou2006 for example, extended the \lasso with the adaptive \lasso algorithm which introduce a weighting term for each model paramter that, if picked carefully, will overcome the shortcomings of \lasso and that the weights should be based on properties of the data.

In [[cite:Julius2009]] a structural constraint was introduced to the \lasso penalty derived from /a priori/ knowledge where structure could be specified as being there or not there, positive or negative or uncertain.
An additional constraint was introduced in [[cite:Zavlanos2011]] where stability of the inferred network was ensured.
In both cases a model similar to the one introduced in section [[ref:sec:linear_models]] was used, with a [[gls:ss]] assumption. 

# [[cite:Nordling2013phdthesis]]

# [[cite:Tegner2003]] Don't know how to use this.

# [[cite:Goncalves2008]] Not sure why this is here.

# [[cite:Ng2004]] L1 vs L2

**** Model selection
To choose a "good" model when infeering networks is not trivial.
\lasso produces a range of different models depending on the penalisation paramter $\zeta$.

As mentioned in section [[ref:sec:linear_penalty]], overfitting is an issue when the data is noisy.
To measure the performance of a network one can calculate the weighted [[gls:rss]],
#+begin_src latex :exports results :results latex
\begin{equation}\label{eq:wrss}
  \chi^2(df) \sim \text{W}\RSS(\mA_f) = (\by-\mA_f^{-1}\bp)^T W^{-1} (\by-\mA_f^{-1}\bp)
\end{equation}
#+end_src
\noindent
where $\mA_f$ denotes any network arrived at by any function, with co-variance matrix $W$ of the measurements.
If the errors in $\by$ are \iid and normally distributed, $\normall$ with mean $\mu$ and variance, $\lambda$, then the weighted [[gls:rss]] follows a [[gls:chi2]] distribution with $df$ degrees of freedom[[cite:Aster2005,Andrae2010]].
It is also possible to compare models to determine if one model is significantly better than another.
The ratio of two reduced [[gls:chi2]] distributions with degrees of freedom, $df_1$ and $df_2$,
#+begin_src latex :exports results :results latex
\begin{equation}
  R = \frac{\chi^2_1/df_1}{\chi^2_2/df_2} = \frac{\chi^2_1 df_2}{\chi^2_2 df_1}
\end{equation}
#+end_src
will follow another F distribution with parameters $df_1$ and $df_2$.
And a statistical test can be made to determine how much better one model is over the other [[cite:Aster2005]].

To circumvent the over-fitting problem, one might employ a [[gls:cv]] approach.
[[gls:cv]] means leaving out a part of the data, fitting the model to the remaining that and calculating ([[ref:eq:wrss]]) or simply the RSS on the left out data.
This procedure is repeated for different portions of the data and the error is calculated each time.

# Model selection
Due to the statistical properties of the weighted [[gls:rss]] it is suitable for goodness of fit testing.
If the error is significantly larger than expected the model is discarded.

The prediction error approach is used in the Inferelator[[cite:Bonneau2006]], a network inference framework, together with a [[gls:cv]] scheme to select a model with sufficiently good performance.
The common assumption that [[glspl:grn]] are sparse is used and motivates a selection of a prediction error one standard deviation above the minimum prediction error for selecting the network.


Two other approaches for model selection are [[gls:bic]] and [[gls:aic]][[cite:Akaike1973_with_commentary]].
Both approaches is based on the likelihood function, the [[gls:bic]],
which can be written as
#+begin_src latex :exports results :results latex
\begin{equation}
  \text{BIC} = m \ln\left(\frac{\text{RSS}}{m}\right) + k \ln(m)
\end{equation}
#+end_src
where $m$ is the number of data points, and $k$ the number of free parameters to be estimated.
An alternative form can be written as
#+begin_src latex :exports results :results latex
\begin{equation}
  \text{BIC} =  \chi^2 + df \ln(m)
\end{equation}
#+end_src
\noindent
where [[gls:chi2]] is the chi square distribution with $df$ degrees of freedom[[cite:Should_be_one_here_from_wikipedia]].

Both the [[gls:bic]] and [[gls:aic]] makes a trade of between model predictability and model complexity.
Both methods have been shown to lack in some regards when compared to \eg [[gls:cv]] [[cite:Thorsson2005]].

**** Network inference challenges                                  :noexport:
# CHECK TORBJORNS THESIS PAGE 28!!!
# SIC
[[cite:Zhao2006]]

**** Inverse problems

[[citet:Aster2005]] describes the nature of the inverse problem, which arises when one tries to estimate model parameters based on measured data or observations related to some independent variables.
This includes the network inference problem and relates the inference problems sensitivity to noise.

Looking at equation [[ref:eq:ls]] we can decompose matrix $\mY =\mU \mSigma \mV^T$ which is just a linear combination of the singular values $\glssymbol{sigma}_k$ and the singular vectors, $\bv_k \bu_k^T$, where $k$ is the specific [[gls:sigma]].
Now the inverse of $\mY$, can be written as another linear combination of these entities,
#+begin_src latex :exports results :results latex
\begin{equation}
  \mY^{\dagger} \equiv \sum_{k=1}^n \frac{1}{\sigma_k}\bv_k \bu_k^T
\end{equation}
#+end_src
\noindent
which means that the singular value that affects the estimate of ([[ref:eq:ls]]) is the smallest singular value of $\mY$.
The smallest singular value represents the direction in the data with the least variation and least information, meaning that the influence of the noise $\mE$ is potentially substantial as the noise corrupts the smallest variation easier.
# any citations?

# discrete inverse problem = parameter estimation problem NOT model identification problem.(maybe only indirectly)

**** Observability                                                 :noexport:



# observability 4.2
# Identifiability
#
# Experimental design
# Fisher information matrix 3.1
#

[[cite:Kremling2007]]

# Strong irrepresentable condition

# Robust network inference

# Observability and controllability  [[cite:Kremling2007]]
# May be in relation to experimental design

** Network inference -- community efforts
Network inference have collected its tools from various scientific disciplines and been applied in several.
A scientifically diverse group of individuals constitutes the network inference community.

In this section I will describe some of the efforts,
resources and approaches that has been built around this research field and how they are connected.

*** Benchmarks
Benchmarking can be used as a tool for evaluating the performance of algorithms or methods trying to solve specific problems.
Usually, introducing a new algorithm, for example, demands that the claims made of its usefulness is accompanied by a benchmark,
a test against other competing methods or algorithms[[cite:someonethatintroduceamethod]].
However, it might be the case that new information or better data becomes availible at a later point or that a scope or application for the method is expanded.
Therefore, larger benchmarks are often conducted with a larger scope than provided original analysis[[cite:Bansal2007,Penfold2011]].
These benchmark has the aim of exploring the performance of methods tested under both a realistice and wide range of conditions.

Two classes of data often collected in relation to [[gls:grn]] inference, [[gls:ssd]] and [[gls:tsd]]. Different assumptions follow these different perturbation,
for [[gls:ssd]] one needs to measured and perturb every gene to be included in the inferred network, see [[ref:equ:lin_sys]].
For [[gls:tsd]] not all genes needs to be perturbed but one needs to capture enough data points as to capture the regulatory effects in short an long term[[cite:Hecker2009]]. 

One can focus on one of these data types when benchmarking algorithms \eg [[gls:tsd]] cite:Ward2009,Narendra2011 or mix different approaches that use both types of data[[cite:Bansal2007,Penfold2011]]. 

Another feature of the data is the underlying model assumptions.
To make the data more realistic a model based more closely on the underlying theory of how the system operates might be used.
Different model assumptions demands different types of data whether it is to simulate [[gls:insilico]] data or to decide what data needs to be collected from an [[gls:invivo]] setup[[cite:Gardner2005]]. For example, if we consider boolean networks. If the regulatory structure of the network is such that a gene can not be "turned on" one can not collect all different combinations of input required to make a truth table for the inference the more regulators the more risk that not all combinations can be realised trivially and the more data needs to be collected.

The [[gls:dream]] challenge is a community effort and competition that aims at combining the previously mentioned features of benchmarking in addition to including a large cotributing community[[cite:Marbach2012]].
The challenges goes back to 2007 and has evolved over time.
The [[gls:dream]] challenge is split in to several different challenges where one ore more are focused on network inference, or identifying unknown regulatory interactions with the help of data and a partly complete network.
The challenges present a mix of [[gls:insilico]] and [[gls:invivo]] data and with some exceptions makes the data available for use when the challenge have finished for use in other works[[cite:Folch-Fortuny2015]].
# May be add more examples than one.

Another core part of any benchmark is how to evaluating the performance of an algorithm being tested and evaluating strengths and weaknesses of methods and appraoches.
As the core aim of network inference is to fined the regulatory structure of the [[gls:grn]] one usually test for if an algorithm can distinguish between [[gls:tp]], [[gls:fp]], [[gls:tn]] and [[gls:fn]],
where positive represent a link and negative the absence of a link.
True and false represents whether the classification an inference method have made of if the link should be present or not is true or false.
These measures are usually summarised in to a more easily enterpratable form, such as a fraction of the measures that range between 0 and 1, \eg sensitivity $=\frac{TP}{TP+FN}$, precision $=\frac{TP}{TP+FP}$, specificity $=\frac{TN}{TN+FP}$ and negative prediction value $=\frac{TN}{TN+FN}$ [[cite:Bansal2007]].
What one would like is a single number that represents the performance and is easily compared and understood. The  [[gls:auroc]] and  [[gls:aupr]]  is used in many benchmarks, see for example,
# Explain these more.
[[cite:Narendra2011,Marbach2010,Marbach2012]].
Some examples of incorporating sign of the link has been made[[cite:Hache2009]].
Which means extending the binary classification in to a more complex structure where you take in to account a link which are inferred but with the wrong sign.

[[citet:Cantone2009]] generated an [[gls:invivo]] data set from an engineered network. The network was tuned so that the interactions would be known and the network was perturbed and the response was measured both for [[gls:ss]] and [[gls:tsd]]. The purpose of this data set was to be able to benchmark methods on a realistic true model with actual measured data.
Even during these conditions it is shown that inferring the true network is difficult[[cite:Penfold2011]].

*** Data and experiments, \insilico vs \invivo
:PROPERTIES:
:CUSTOM_ID: sec:data_experiments
:END:

A large collection of toolboxes has been developed aimed at systems biology research.
which focuses mainly on creating simulated [[glspl:grn]] see for example: [[cite:thispackaage_thatpackage_theotherpackage]].
This is a response to the fact that regulatory networks in biology are generally lacking in information and are one of the least available networks types[[cite:Barabasi2011]].
This has to be paired with available data suitable for network inference under stable enough conditions so that the change in the states observed in the data is a consequence of regulatory effects and not for example the network being in a specific mode or that a part of the network is missing, which can happen if genes are deleted.
Toy models and [[gls:insilico]] generated data have been shown to be a good proxy for estimating performance off network inference algorithms[[cite:Bansal2007]]. [[Gls:insilico]] models have been used to predict and tune optimal evolutionary growth through the metabolic network[[cite:Ibarra2002]].
It is also beneficial if one can prepare or extend experimental procedures by first running simulations on a computer and many times necessary to be able to maximise the usefulness of the [[gls:invivo]] experimental output[[cite:Nordling2013phdthesis]].

Another benefit of being able to use simulated data is that it is easier to explore and examine a wider range of properties of both network and data.
Networks with with different structure and different amounts of motifs can be generated and methods can be tested on how they perform during specific conditions[[cite:Marbach2012]].

If some knowledge exists, even partial knowledge, one can incorporate this information to get more realistic data sets, such as known regulatory networks[[cite:Schaffter2011]].

For [[gls:invivo]] generated data there is no need to worry about "realistic" models or experimental conditions, such as realistic noise models or system response patterns or network structure.
Therefore it is desired to generate data in living systems even when testing methods.
The drawback being that a gold standard might not exist to estimate performance.
There has been several successful attempts of both data generation and inference including [[gls:invivo]] data and a proposed true [[gls:grn]] [[cite:Gardner2003,Cantone2009,Lorenz2009]].

# cites Ljung1999 for identification and perturbation response setup. [[cite:Ljung1999]]

*** Tools of systems biology
In a research field that rely heavily on computation it's unavoidable that a large number of lines of code and data is generated.
Except the scientific knowledge generated with these tools, they are themselves a valuable contribution to the body of scientific knowledge.
# A reference I haven't found yet: Schmidt 2006. "Information technology in systems biology."
In this section I will try to collect a number of different tools used in system biology with the aim of helping with [[gls:grn]] inference.
The tools needs to cover mainly three different areas.
(i) Algorithms and methods, which is the main are of tools.
Without them the goals of systems biology could not be reached.
(ii) Data formats and communications.
To be able to share data and communicate results and information, common data formats should be developed.
(iii) Simulation and benchmarking.
These tools should accompany any inference method so that it can easily be evaluated.

Table [[ref:tab:inference_methods]] gives an overview of a number of inference methods.
For each method the short and ling names are given, if available. The goal of the algorithm together with the modelling scheme is also listed.

Table [[ref:tab:insilico_modelling]] lists a number of tools used for \insilico simulation and modelling. As detailed in section [[ref:sec:data_experiments]] the demand for testing the array of network inference methods is facilitated by tools that can generate simulated data and networks.

Table [[ref:tab:system_communication]] list tools and formats for sharing and communicating systems biological data and knowledge.

# [[cite:Bonneau2008]]

#+BEGIN_LATEX
\begin{landscape}
#+END_LATEX
#+CAPTION: List of network inference methods. Short name is the name usually used to refer to the method. 
#+label: tab:inference_methods
#+ATTR_LATEX: :environment longtable
| Reference            | Short Name  | Name                                                             | Model Scheme             | Goal                                          | Directed Edges (y/n) | Uses Perturbations (y/n) |
|----------------------+-------------+------------------------------------------------------------------+--------------------------+-----------------------------------------------+----------------------+--------------------------|
|                      |             |                                                                  |                          |                                               |                      |                          |
| [[cite:DiBernardo2005]]  | MNI         | Mode-of-action by network identification                         |                          | Determine drug targets                        | y                    | y                        |
| [[cite:Julius2009]]      |             |                                                                  | ODEs                     | GRN                                           | y                    | y                        |
| [[cite:Greenfield2010]]  | MCZ         | Median Corrected Z-Scores                                        | Information-theoretical  | GRN                                           | y                    | y                        |
| [[cite:Pinna2010]]       |             | Graph-based method                                               | Z-score-based            | GRN                                           | y                    | y                        |
| [[cite:Grimaldi2011]]    | RegnANN     | Reverse engineered gene networks with artificial neural networks | neural networks          | GRN                                           | y                    | y                        |
| [[cite:Zavlanos2011]]    |             | Inferring stable genetic networks from steady-state data         | linear dynamical systems | GRN                                           | y                    | y                        |
| [[cite:Xiong2012]]       |             | Method with regression and correlation                           | Info-theoretic / LDS     | GRN                                           | y                    | y                        |
| [[cite:Gardner2003]]     | NIR         | Network identification by multiple regression                    | ODEs                     | GRN & identify drug targets                   | y                    | y                        |
| [[cite:Friedman2010]]    | Glmnet      | Lasso (L1) and elastic-net regularized generalised linear models |                          | Linear regression                             | y                    | y                        |
|                      | LSCO        | least squares with cutoff                                        |                          |                                               | y                    | y                        |
| [[cite:Faith2007]]       | CLR         | Context likelihood of relatedness                                | Information-theoretical  | GRN                                           | y                    | y                        |
| [[cite:Jornsten2011]]    | EPoC        | Endogenous perturbation analysis of cancer                       |                          | GRN                                           | y                    | y                        |
| [[cite:Shih2012]]        |             | Single source k-shortest paths algorithm                         | graph theory             | GRN                                           | n                    | y                        |
| [[cite:Menendez2010]]    | GMRF        | Graphical lasso with Gaussian Markov Random Fields               | relevance based          | GRN                                           | n                    | y                        |
|                      |             | Adaptive lasso                                                   |                          |                                               |                      |                          |
|                      |             | SCAD penalty                                                     |                          |                                               |                      |                          |
| [[cite:Nordling2011]]    |             | Rank Reduction                                                   | linear ODE               | GRN                                           | y                    | y                        |
| [[cite:Wang2012]]        |             |                                                                  |                          | GRN                                           |                      |                          |
| [[cite:Nordling2013]]    | RNI         | Confidence based Robust Network Inference                        |                          | GRN                                           | y                    | y                        |
|                      |             | Cyclic coordinate descent Lasso solver                           |                          |                                               |                      |                          |
| [[cite:Cosgrove2008]]    | SSEM-Lasso  | Sparse simultaneous equation model – Lasso regression            |                          | Determine drug targets                        | y                    | y                        |
|                      | TSNI        | Time series network inference                                    |                          |                                               |                      |                          |
| [[cite:Oates2012]]       |             | Bayesian network using Goldbeter Koshland kinetics               | Bayesian                 | Protein-signalling network                    | y                    |                          |
| [[cite:Lauria2009]]      | NIRest      | NIR with perturbation estimate                                   | ODEs                     | estimate P, identify GRN                      | y                    | n (it estimates them)    |
| [[cite:Margolin2006]]    | ARACNE      | Algorithm for the Reconstruction of Accurate Cellular Networks   | Information-theoretical  | GRN                                           | n                    | n                        |
| [[cite:K2012]]           | ANOVA       | ANOVA                                                            | ANOVA                    | GRN                                           | y                    | n                        |
| [[cite:Huynh2010]]       | GENIE3      | Tree-based method                                                | Tree-based               | GRN                                           | y                    | n                        |
| [[cite:Castelo2009]]     | Qp-graphs   | Q-order partial correlation graphs                               | graph theory             | GRN                                           | y                    | n                        |
| [[cite:Ambroise2012]]    | TNIFSED     | Supervised transcriptional network inference                     | supervised               | Assign probability of being target of each TF |                      | n                        |
|                      |             | from functional similarity and expression data                   |                          |                                               |                      |                          |
| [[cite:Mordelet2008]]    | SIRENE      | Supervised inference of regulatory networks                      | supervised               | Assign targets to TFs                         |                      | n                        |
| [[cite:Sun2007]]         | TRND        | Transcriptional regulatory network discovery                     | Bayesian                 | Assign targets to TFs                         |                      | n                        |
| [[cite:de2012]]          | BC3NET      | Bootstrap aggregation ensemble C3NET                             | Information-theoretical  | GRN                                           | n                    | n                        |
| [[cite:Altay2010]]       | C3NET       | Conservative causal core network inference                       | Information-theoretical  | GRN                                           | n                    | n                        |
| [[cite:Friedman2007]]    |             | Graphical lasso                                                  |                          | Sparse inverse covariance estimation          | y                    |                          |
| [[cite:Bonneau2006]]     | Inferelator | the Inferelator                                                  | ODEs                     | GRN                                           | y                    | y                        |
| [[cite:Gevaert2007]]     |             |                                                                  | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Husmeier2007]]    |             |                                                                  | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Lahdesmaki2008]]  | RJMCMC      | Reversible jump Markov chain Monte Carlo                         | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Nelander2008]]    | CoPIA       | Combinatorial Perturbation-based Interaction Analysis            | ODEs                     | GRN                                           | y                    | y                        |
| [[cite:Yip2010]]         |             |                                                                  | ODEs                     | GRN                                           | y                    |                          |
| [[cite:Yu2004]]          | BANJO       |                                                                  | Bayesian                 | GRN                                           | y                    | y                        |
| [[cite:Djebbari2008]]    |             | Seeded Bayesian networks                                         | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Aijo2009]]        |             | Dynamic Bayesian network inference with Guassian processes       | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Chai2012]]        |             | Dynamic Bayesian network inference with imputed missing values   | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Wang2010]]        |             | [Boolean] Process-based network decomposition                    | Boolean                  | GRN or motifs                                 | y                    | n                        |
| [[cite:Schultz2012]]     | DREM        | Dynamic Regulatory Events Miner                                  |                          | More TF-target and timing than GRN            |                      | n                        |
| [[cite:Hache2007]]       | GNRevealer  | Reconstructing GNRs with neural networks                         | neural networks          | GRN                                           | y                    |                          |
| [[cite:Kabir2010]]       |             | Linear time-variant method                                       |                          | GRN                                           | y                    |                          |
|                      |             | using self-adaptive differential evolution                       |                          |                                               |                      |                          |
| [[cite:Gustaffson2010]]  | RLAD        | Regularized Least-squares Absolute Deviation,                    | ODE's                    | GRN                                           | y                    | y                        |
|                      |             | or Least-squares with elastic net penalty                        |                          |                                               |                      |                          |
| [[cite:K2010]]           | PNFL        | Petri net with fuzzy logic                                       | petri net                | GRN                                           | y                    | y                        |
| [[cite:Grzegorcyck2012]] |             | Non-homogeneous dynamic Bayesian network                         | Bayesian                 | GRN                                           | y                    |                          |
| [[cite:Wu2011]]          | SSM         | State space model w/hidden variables                             | state space model        | GRN                                           | y                    | n                        |
| [[cite:Penfold2012]]     |             | Hierarchical non-parametric Bayesian                             | Bayesian                 | GRN                                           | y                    | y                        |
| [[cite:Bock2012]]        |             | Hub-centered GRN inference using automatic relevance             | Bayesian                 | GRN or hubs                                   | y                    | n                        |
| [[cite:Layek2011]]       |             | Boolean networks represented by Karnaugh maps                    | Boolean                  | GRN                                           | y                    | n                        |
| [[cite:Kimura2012]]      | LPM         | Linear program machine-based S-system GRN inference method       | S-system                 | GRN                                           | y                    | n                        |
| [[cite:Alakwaa2011]]     | BicAT-Plus  | Bi-clustering with Bayesian for GRN inference                    | Bayesian                 | GRN                                           | y                    | n                        |
| [[cite:Li2011]]          | DELDBN      | Differential Equation-based Local Dynamic Bayesian Network       | Dynamic Bayesian         | GRN                                           | y                    |                          |
| [[cite:Lu2011]]          |             | Delayed feedback                                                 | control theory           | GRN                                           | n                    | y                        |
| [[cite:August2009]]      |             | Linear program biochemical network inference                     |                          | GRN                                           | y                    |                          |
| [[cite:Yuan2011]]        |             | Robust network structure reconstruction                          | ODE's/LDS                | GRN                                           | n                    | y                        |
| [[cite:Zhang2012]]       | NARROMI     | Noise and redundancy reduction technique using                   | Info-theoretic and ODEs  | GRN                                           | y                    | ?                        |
|                      |             | recursive optimisation and mutual information                    |                          |                                               |                      |                          |

#+BEGIN_LATEX
\end{landscape}
#+END_LATEX

#+CAPTION: Simulation and benchmark tools used for network inference
#+label: tab:insilico_modelling
| Reference             | Short name      | modelling            |
|-----------------------+-----------------+---------------------|
| [[cite:Schaffter2011]]    | GeneNetWeaver   | Non-linear          |
|                       |                 | regulatory networks |
| [[cite:Villaverde2015]]   | BioPreDyn-bench |                     |
| [[cite:Hache2009b]]       | GeNGe           | Non-linear          |
|                       |                 | regulatory networks |
| [[cite:VandenBulcke2006]] | SynTReN         | Non-linear          |
|                       |                 | regulatory networks |
| [[cite:DiCamillo2009]]    |                 | Non-linear          |
|                       |                 | regulatory networks |


#+CAPTION: Tools for used in systems biology to facilitate communication and results
#+label: tab:system_communication
| Reference         | tool       | usage                      |
|-------------------+------------+----------------------------|
|                   | SBML       | data format                |
|                   | CellML     | data format                |
|                   | SimBiology | simulation and programming |
| [[cite:Schmidth2006]] | SBToolbox  | simulation and programming |
|                   | Copasi     |                            |
|                   | Gepasi     |                            |

* Present investigations

** Model selection based on minimum prediction error (PAPER I)
:PROPERTIES:
:CUSTOM_ID: sec:paper1
:END:

Optimal model selection problem is as of yet an open problem.
How to properly choose a specific set of parameters for the network inference algorithms,
to determine the sparsity, has not been solved and no optimal method has been put forward.

Some classical alternatives proposed are the [[gls:bic]] and [[gls:aic]] which both trade-of prediction and complexity to find an optimal model.
as well as cross validation and select based on minimisation of the [[gls:rss]].

All these methods for model selection are motivated by the fact that data is recorded with noise and that over-fitting the model then is always a risk.
They have been shown to perform well asymptotically with \eg the number of samples[[cite:Stoica2004]]

In this paper we studied the effects on model selection when the data had a varying degree of informativeness.
Informativeness was defined based on the optimal performance of the inference method on the data when compared to a gold standard.
If the performance was matched the gold standard optimal then the data set was considered informative,
if the performance was non optimal but better then random the data set was deemed partly informative and if the performance was no better than random the data was labeled uninformative. We used a specific method, [[gls:rni]],
to determine informativeness of the data.
The informativeness was varied based on two factors, (i) the properties of the network and experimental design, (ii) the [[gls:snr]].

The data used was generated [[gls:insilico]] as this had been utilised with success previously and been shown to be an good indication of how a method would perform on other data [[cite:Menendez2010,Bansal2007]].

We determined two additional steps that should be utilised when solving a network inference problem.

First we showed that to be able to utilise a leave out cross validation approach, or as we employ it here, a leave one out cross optimisation (LOOCO), one needs to test for dependence of the sample on the rest of the data and only include the sample in the left out group if it is sufficiently described by the data that is going to be used to infer a network.
The reason for this is that a network inferred from data that has know description of a sample can not make any predictions about that data.

Secondly we introduced a step of re-estimating the parameters returned from an inference algorithm.
Here we argued that because the consequence of introducing a bias due to the penalty used in many inference method,
to be able to combine model selection and data fitting,
the parameters of the model are not the maximum likelihood estimate anymore which may skew the [[gls:rss]] for the predictions.
The algorithm for re-estimating the parameters is a [[gls:cls]] algorithm.
[[gls:cls]] preserves the structure of the network while refitting the parameters.

We showed that if the data was uninformative we can not make a useful model selection,
while if the data was partly informative or informative,
the model selection based on the [[gls:rss]] would maximise the true positive (TP) while minimising the false positive (FP).
Giving our selection method a bound where the minimum [[gls:rss]] would not be achieved when any TP link would be removed.

*** Future perspective
We showed that conceptually our approach worked. However we did not investigate the performance in general and what the behaviour of our approach would be for a wide variety of data properties.
Several technical additions to a new study would greatly benefit this investigation.

We do not test the [[gls:bic]] and [[gls:aic]] selection methods.
Both of these methods are dependent on the likelihood function and should therefore also have their performance influenced by our additional steps. Specifically the introduction of [[gls:cls]].

The [[gls:rss]] was calculated as the mean [[gls:rss]] over all the selected leave out samples.
A new study would greatly benefit from utilising the statistical properties of the [[gls:rss]], such as the fact that if the error of the measurements are assumed to be normal, the [[gls:rss]] will follow a [[gls:chi2]] distribution.
With some care when estimating the degrees of freedom for each model[[cite:Andrae2010]] an exclusion step would then be done where all models not passing a goodness of fit test would be excluded as candidate networks.
The result would be a set of candidate networks in which we could in theory pick any of them.
We would expect, though, that we would pick the sparsest candidate with the argument that [[glspl:grn]] are, in general, sparse.

** Including prior information to enhance network inference accuracy (PAPER II)
:PROPERTIES:
:CUSTOM_ID: sec:paper3
:END:

In this paper we investigated whether or not one could improve inference methods with the help of inclusion of prior information.

It is often the case that when trying to solve a network inference problem within biology that the data is under-determined. Meaning that a unique solution can not be found.
It is also of the case when dealing with biological data that the [[gls:snr]] is low or that very few replicates have been recorded.

In both these situations it may be beneficial to include prior information. In the first case, if we include prior structural knowledge of the regulatory interactions, we can constrain the problem to a subset of interactions so that it no longer becomes under determined.
In the second case we might have knowledge that we are confident about of which interactions are more likely to exist and that can help guide an inference method when the data is of lower quality.

In this paper we investigated the latter case.

Available on-line there are a number of databases containing functional associations collected from a wealth of sources with a number of different evidence types[[cite:Szklarczyk2011,Schmitt2014]].

Incorporating a prior in the network inference pipeline can be done in a number of ways.
In this study we focused on incorporating functional associations which are usually presented by a number of the confidence that is associated with the link.
These associations are by their nature undirected and it is often unknown if they are representing direct or indirect links, and if they are parallel or serial.
Therefore we opted for including the confidence of links as weights inversely proportional to the confidence, meaning that links that have a high confidence gives a low wight to the associated penalty term giving the link a higher chance of being selected.
For example, if the confidence is low but the data indicates a strong link, both the effects are traded against each other.
By incorporating the associations as weights it explicitly gives the possibility of the data to speak as well.

To test the performance of incorporating a prior in to the network inference pipeline a number of different networks and [[gls:insilico]] data sets where generated.
Two different models of system and data was used,
a linear system model and a non-linear system model[[cite:Schaffter2011]].

Priors incorporation performance were tested by changing the prior accuracy.
Accuracy where changed by controlling whether or not the confidence for a true link was drawn from the a distribution of low confidence associations and a negative link was drawn from a distribution of high confidence links.

When the data was un-informative an improvement with the prior could be observed if the prior was more correct than not.
For data generated with the linear model the prior needed on average to be more correct than for a non-linear model.
This also scaled with the [[gls:snr]] of the data sets which in general was higher for the linear system vs non-linear.

We also wanted to test the prior incorporation on real data and used a data collected from \yeast with the gold standard network was collected from the Yestract database[[cite:Teixeira2013]].
To estimate the performance we checked the difference over all sparsity levels the inference method returned.
We did this to remove the factor of trying to pick the correct sparsity for the network inference method.

An improvement with the prior could be observed over almost all sparsity levels with an emphasises on the sparser range of the spectrum where we would assume that the optimal network should be found.

*** Future perspective

One question that was not answered in this paper was, at what quality of the data is it useful to include a prior?
While the accuracy of the prior was investigated, the range of [[gls:snr]] was not.
This could serve useful when the accuracy of the prior or the nature of the prior \eg being undirected, might obstruct the inference algorithm.

Due to the evidence types of the prior, the associations might be indirect.
A modified algorithm could make use of this information and instead of inserting a confidence as a weight of an interaction the association could be incorporated in a way so that the association is preserved in the inferred network even though it is not direct, reflecting the nature of the association.

** Practical workarounds for the pitfalls of L1 penalised regression methods (PAPER III)
:PROPERTIES:
:CUSTOM_ID: sec:paper2
:END:

It is know that performance of penalised regression methods, specifically the $L_1$, penalised \eg [[gls:lasso]], algorithm perform poorly under some conditions [[cite:putsomethinghere]].
Sometimes referred to as the predictors having a high co-linearity.
In systems theoretic terms this can be quantify by calculating the condition number [[gls:k]] of the data set.
An ill conditioned matrix has in general a high degree of co-linearity.
# Better check with Torbjorn if this is correct.

The observation here is that even when the data is informative,
defined as in PAPER I [[ref:sec:paper1]],
the $L_1$ penalised methods performs as if the data was only partly informative even when we act as if we had expert knowledge when selecting the optimal network produced by the inference method.

The performance of these types of inference method have been investigated and been shown to be a function of the data and network[[cite:SIC,others,gnw_and_dream_motif,fisher_inf_matrix]].
The issue with these results is that they are impractical in reality as we don't know the network structure beforehand and in some cases we would arrive at the wrong conclusions if we would use the wrong network structure to calculate them.

We show that a proxy for predicting the performance of an inference method is to investigate the properties of the data,
specifically the condition number [[gls:k]] and the [[gls:snr]].

We use synthetic data to vary the properties of both network and [[gls:insilico]]  expression data.
We constructed the data so that the properties ranged over known values of properties for real biological data sets.
The properties of the expression data is highly dependant on the network properties but they can be tuned depending on the experimental design[[cite:Nordling2009]].
This is demonstrated with 3 different experimental designs.
Two of the approaches could easily be employed in practise and show specifically that these designs made the data properties highly dependent on the network properties.
The third approach would be more involved to implement in practise and aimed at minimising the [[gls:k]] for the expression matrix.
It demonstrated clearly that decoupling the data and network properties and tune the input so that the data properties would approach more desired states would greatly enhance the performance of the inference and network construction.

While few real data set exists with sufficient data to quantify the properties used in this work and simultaneously have a reference regulatory network,
we picked one data set derived from over expression experiment with three proposed regulatory network derived experimentally.
It was show that by calculating the properties of the data one could predict the performance of the inference methods based on the [[gls:insilico]] data.

*** Future perspective

# TLS

** GeneSPIDER, a software package for a simplified network inference pipeline (PAPER IV)
:PROPERTIES:
:CUSTOM_ID: sec:paper4
:END:

*** Future perspective

* Backmatter                                                        :notitle:
#+LATEX: \backmatterSU

* Sammanfattning
#+LATEX: \selectlanguage{swedish}
En kort summering av avhandlingen p\r{a} svenska om avhandlingen \"ar skriven p\r{a}  ett annat spr\r{a}k.

\r{a} \"a \"o
#+LATEX: \selectlanguage{english}
* References                                                        :notitle:
#+LATEX: \renewcommand{\bibname}{References} % changes the header from Bibliography to References
#+LATEX: \begin{scriptsize} % tiny(5) < scriptsize(7) < footnotesize(8) < small (9)

[[bibliographystyle:plainnat]]
[[bibliography:~/research/bibliography.bib,./references.bib]]

#+LATEX: \end{scriptsize}

* Glossaries                                                        :notitle:
#+begin_src latex :exports results :results latex
\printglossary
#+end_src

* COMMENT Ideas and structure

** My publication

[[cite:Tjarnberg2013]]

[[cite:Studham2014]]

[[cite:Tjarnberg2014]]

[[cite:Tjarnberg2015-unpublished]]


** As of yet unplaced citations,

[[cite:Tegner2007]] Perturbations 

** DONE Check the GeneSPIDER for the network generation reference
To answer the question; what is small world networks.
[[citet:Prettejohn2011]], section 2.6 specifically sais it's not clear what small world mean.

** TODO Comments from [[cite:Zavlanos2011]] about causal models, specifically differential models, should be viewed and incorporated.
- as well as this:
  steady-state measurements (Gardner et al., 2003; Julius et al., 2009; Tegner et al.,2003) or dynamic time-series (Amato et al., 2007; August & Papachristodoulou, 2009; Bansal et al., 2006; Cinquemani et al., 2009; Papachristodoulou & Recht, 2007; Porreca et al., 2008; Sontag et al., 2004; Srividhy et al., 2007)


** Comments from specific sections now removed to here

*** Linear models
linear models should not be discarded until they are not suficiently well describing the data, ref model selection, where if the statistical test of RSS(?) is not fitting the data while the data quality is high, high \eg SNR then can one discard a simple model.

# What citation!
[[cite:Crampin2006]]
Model formalism,# Near steady state linear system. Also a nice figure of different levels of network representation, figure 2.

Why use linear models?

Few degrees of freedom/ parameters.
demands "little" data.
Are easy to model.
Linear model and a lot more, this is a review [[cite:DeJong2002a]]

*** Network inference
# see: Zavlanos et al for the quote "The ensemble of both classes form the so-called genetic network identification problem." first section. The following text is interesting as a reference.

*** Model selection
# specifically chi2 and  f distribution and 2 and 1 norm RSS!!! Chapter 2 I think also page 13 example 1.1.
[[cite:Aster2005]]

# RSS, Goodness of fit test
# General system and statistical learning Least squares, RSS page 12

# BIC
# AIC
# [[cite:Umezu2015]] [[cite:Yang2005]] Not yet read, check them out at work

*** Tools
# Tools and formats for systems biology [[cite:Kremling2007]]

** Figures
1. Based on figure 2 in [[cite:Gardner2005]]
2. Abstract network model based on [[cite:Crampin2006]]

* Setup code                                                       :noexport:
Code used when exporting to latex
#+name: setup
#+begin_src emacs-lisp :results silent :exports none
(unless (find "thesis-book-SU" org-latex-classes :key 'car
              :test 'equal)
  (add-to-list 'org-latex-classes '("thesis-book-SU" "\\documentclass[11pt]{book}
\\usepackage{thesisStyleSU}
[NO-DEFAULT-PACKAGES]
[PACKAGES]
[EXTRA]"
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}"))))
#+end_src

[[https://emacs.stackexchange.com/questions/9492/is-it-possible-to-export-content-of-subtrees-without-their-headings][source]]. This is currently incompatible with the latest org-mode
#+name: test1
#+begin_src emacs-lisp :results silent :exports none
(defun org-remove-headlines (backend)
  "Remove headlines with :notitle: tag."
  (org-map-entries (lambda () (let ((beg (point)))
                                (outline-next-visible-heading 1)
                                (backward-char)
                                (delete-region beg (point))))
                   "noexport" tree)
  (org-map-entries (lambda () (delete-region (point-at-bol) (point-at-eol)))
                   "notitle"))

(add-hook 'org-export-before-processing-hook #'org-remove-headlines)
#+end_src

This is untested.
#+name: test2
#+begin_src emacs-lisp :results silent :exports none
(defun sa-ignore-headline (contents backend info)
  "Ignore headlines with tag `ignoreheading'."
  (when (and (org-export-derived-backend-p backend 'latex 'html 'ascii)
          (string-match "\\`.*nononotitle.*\n"
                (downcase contents)))
    (replace-match "" nil nil contents)))

(add-to-list 'org-export-filter-headline-functions 'sa-ignore-headline)
#+end_src

** File Local variables                                            :noexport:
### Local Variables:
### ispell-local-dictionary: "british"
### 
### End:
